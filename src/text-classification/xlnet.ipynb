{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "xlnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7VWdRufO4sv",
        "outputId": "47aae071-1a00-475e-cb80-8cbf50fa322c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwBG6Z2yPLcO",
        "outputId": "44d5606f-caed-4462-c872-7ef56c04820f"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md         run_glue_no_trainer.py  run_xnli.py\n",
            "requirements.txt  run_glue.py             \u001b[0m\u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01SjlIgLPjJP",
        "outputId": "74189519-61d7-4baf-e323-35c100708037"
      },
      "source": [
        "cd gdrive/\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eDd0uOQPmTF",
        "outputId": "593647bd-c001-4e80-f263-c7b977b227d6"
      },
      "source": [
        "cd MyDrive/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqRStPxCP2Fw",
        "outputId": "826393a2-1a4d-4d14-8dcb-0bcb6bb9bfac"
      },
      "source": [
        "cd deep\\ learning"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/deep learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiFKSJnIP6tN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27e0837-82fa-4af5-d0ea-63220f451f12"
      },
      "source": [
        "!pip install transformers datasets -qq\n",
        "!pip install wandb -qq"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 270 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 53.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 63.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 50.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 123 kB 55.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 40.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 294 kB 49.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 142 kB 53.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 139 kB 64.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 61.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkcalu42QGih"
      },
      "source": [
        "!wget https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py -qq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-s7GWz2oQM1x",
        "outputId": "0fab3f8c-2cf9-4055-ae7b-8927085783b7"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/deep learning'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyfsBi42QbIQ",
        "outputId": "cefbac8e-e9d9-48dd-f7b5-2243f863adab"
      },
      "source": [
        "cd transformers/examples/pytorch/text-classification/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taXL7reJQdNY",
        "outputId": "f796a1b6-7600-4103-e4d4-7dbfc2df1dac"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt9t8NL0ThxH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsOcCedt4J45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289b6bbe-0f5f-4247-96cc-c2ae2f8bf999"
      },
      "source": [
        "%%shell\n",
        "for TASK_NAME in cola sst2 mrpc stsb qqp mnli qnli rte wnli\n",
        "do\n",
        "    echo $TASK_NAME\n",
        "    CUDA_VISIBLE_DEVICES=0 python run_glue.py \\\n",
        "        --model_name_or_path xlnet-base-cased \\\n",
        "        --task_name $TASK_NAME \\\n",
        "        --do_eval \\\n",
        "        --max_seq_length 128 \\\n",
        "        --per_device_train_batch_size 32 \\\n",
        "        --learning_rate 2e-5 \\\n",
        "        --num_train_epochs 3 \\\n",
        "        --output_dir saved_dir/${MODEL_NAME}/${TASK_NAME}/\n",
        "done\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cola\n",
            "09/23/2021 06:50:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/23/2021 06:50:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//cola/runs/Sep23_06-50-04_bb9cb4da2be7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//cola/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//cola/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/23/2021 06:50:05 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8r9uphkv\n",
            "Downloading: 28.8kB [00:00, 22.9MB/s]       \n",
            "09/23/2021 06:50:05 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/6448ad3256e939b3c47a66c9a5d8b8be11ce2f26dbb2b00f3b528bd916924204.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\n",
            "09/23/2021 06:50:05 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6448ad3256e939b3c47a66c9a5d8b8be11ce2f26dbb2b00f3b528bd916924204.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\n",
            "09/23/2021 06:50:05 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpi5nxa5pl\n",
            "Downloading: 28.7kB [00:00, 25.1MB/s]       \n",
            "09/23/2021 06:50:05 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/b78f921f851167a222865867e9ff0d039acd123fb41b2b780654dc73909ed791.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\n",
            "09/23/2021 06:50:05 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b78f921f851167a222865867e9ff0d039acd123fb41b2b780654dc73909ed791.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\n",
            "09/23/2021 06:50:05 - INFO - datasets.load - Creating main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/23/2021 06:50:05 - INFO - datasets.load - Creating specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:50:05 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/23/2021 06:50:05 - INFO - datasets.load - Copying dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/23/2021 06:50:05 - INFO - datasets.load - Creating metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/23/2021 06:50:05 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:50:05 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/23/2021 06:50:05 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/23/2021 06:50:05 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/CoLA.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8s0v4y59\n",
            "Downloading: 100% 377k/377k [00:00<00:00, 11.4MB/s]\n",
            "09/23/2021 06:50:06 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/CoLA.zip in cache at /root/.cache/huggingface/datasets/downloads/bb971ae26c644d1ca0a93f2edb4402d5934802431d3ccce209d74ddeef0c5815\n",
            "09/23/2021 06:50:06 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/bb971ae26c644d1ca0a93f2edb4402d5934802431d3ccce209d74ddeef0c5815\n",
            "09/23/2021 06:50:06 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/23/2021 06:50:06 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/23/2021 06:50:06 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/23/2021 06:50:06 - INFO - datasets.builder - Generating split train\n",
            "09/23/2021 06:50:06 - INFO - datasets.builder - Generating split validation\n",
            "09/23/2021 06:50:06 - INFO - datasets.builder - Generating split test\n",
            "09/23/2021 06:50:06 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 353.17it/s]\n",
            "[INFO|file_utils.py:1664] 2021-09-23 06:50:07,006 >> https://huggingface.co/xlnet-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptjctz0_1\n",
            "Downloading: 100% 760/760 [00:00<00:00, 766kB/s]\n",
            "[INFO|file_utils.py:1668] 2021-09-23 06:50:07,283 >> storing https://huggingface.co/xlnet-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|file_utils.py:1676] 2021-09-23 06:50:07,283 >> creating metadata file for /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:50:07,284 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:50:07,285 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-23 06:50:07,562 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:50:07,840 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:50:07,841 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1664] 2021-09-23 06:50:08,413 >> https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxj7i0_lb\n",
            "Downloading: 100% 779k/779k [00:00<00:00, 2.41MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-09-23 06:50:09,098 >> storing https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|file_utils.py:1676] 2021-09-23 06:50:09,098 >> creating metadata file for /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|file_utils.py:1664] 2021-09-23 06:50:09,382 >> https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptcx28tr3\n",
            "Downloading: 100% 1.32M/1.32M [00:00<00:00, 3.42MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-09-23 06:50:10,073 >> storing https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|file_utils.py:1676] 2021-09-23 06:50:10,074 >> creating metadata file for /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:10,918 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:10,918 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:10,919 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:10,919 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:10,919 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:50:11,197 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:50:11,198 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1664] 2021-09-23 06:50:11,587 >> https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpt586tnve\n",
            "Downloading: 100% 445M/445M [00:11<00:00, 40.4MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-09-23 06:50:23,206 >> storing https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[INFO|file_utils.py:1676] 2021-09-23 06:50:23,207 >> creating metadata file for /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[INFO|modeling_utils.py:1291] 2021-09-23 06:50:23,207 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-23 06:50:24,598 >> Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-23 06:50:24,598 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/9 [00:00<?, ?ba/s]09/23/2021 06:50:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5e7a0db2435b8401.arrow\n",
            "Running tokenizer on dataset: 100% 9/9 [00:00<00:00, 19.67ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/23/2021 06:50:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-74755001608814f1.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 31.33ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/23/2021 06:50:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e7db954e0e2a515d.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 32.31ba/s]\n",
            "09/23/2021 06:50:25 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp3ybs4a0c\n",
            "Downloading: 5.78kB [00:00, 6.66MB/s]       \n",
            "09/23/2021 06:50:25 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/3d00b264fb94fc8d9977c2149d05fc9eb1f8a5e485aadb2eae351dfc9c3a6db4.d4934ed99ee0ef4a5d0d757367c52f72d6ae30a68db09205c01563cab11c5a0d.py\n",
            "09/23/2021 06:50:25 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3d00b264fb94fc8d9977c2149d05fc9eb1f8a5e485aadb2eae351dfc9c3a6db4.d4934ed99ee0ef4a5d0d757367c52f72d6ae30a68db09205c01563cab11c5a0d.py\n",
            "09/23/2021 06:50:25 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/23/2021 06:50:25 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/23/2021 06:50:25 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/23/2021 06:50:25 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/23/2021 06:50:25 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/23/2021 06:50:32 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-23 06:50:32,875 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: idx, sentence.\n",
            "[INFO|trainer.py:2209] 2021-09-23 06:50:32,879 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 06:50:32,879 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2214] 2021-09-23 06:50:32,880 >>   Batch size = 8\n",
            " 99% 130/131 [00:06<00:00, 19.51it/s]09/23/2021 06:50:39 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-23 06:50:39,753 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//cola/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/1ul1r1ry\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065040-1ul1r1ry\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 131/131 [00:10<00:00, 12.78it/s]\n",
            "***** eval metrics *****\n",
            "  eval_loss                 =      0.774\n",
            "  eval_matthews_correlation =     0.0057\n",
            "  eval_runtime              = 0:00:06.86\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =    152.009\n",
            "  eval_steps_per_second     =     19.092\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3966\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065040-1ul1r1ry/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065040-1ul1r1ry/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/loss 0.774\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/matthews_correlation 0.00573\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/runtime 6.8615\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/samples_per_second 152.009\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/steps_per_second 19.092\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/matthews_correlation ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//cola/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/1ul1r1ry\u001b[0m\n",
            "sst2\n",
            "09/23/2021 06:50:53 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/23/2021 06:50:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//sst2/runs/Sep23_06-50-53_bb9cb4da2be7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//sst2/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//sst2/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/23/2021 06:50:53 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/23/2021 06:50:53 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:50:53 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/23/2021 06:50:53 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/23/2021 06:50:53 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/23/2021 06:50:53 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:50:53 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/23/2021 06:50:53 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/23/2021 06:50:53 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/SST-2.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp1w0j3_t6\n",
            "Downloading: 100% 7.44M/7.44M [00:00<00:00, 35.8MB/s]\n",
            "09/23/2021 06:50:54 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/SST-2.zip in cache at /root/.cache/huggingface/datasets/downloads/fda31747115223e6a71a9b791b0723e7b113a7441b9ebc2eacd14d31e2a855ba\n",
            "09/23/2021 06:50:54 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/fda31747115223e6a71a9b791b0723e7b113a7441b9ebc2eacd14d31e2a855ba\n",
            "09/23/2021 06:50:54 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/23/2021 06:50:54 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/23/2021 06:50:54 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/23/2021 06:50:54 - INFO - datasets.builder - Generating split train\n",
            "09/23/2021 06:50:57 - INFO - datasets.builder - Generating split validation\n",
            "09/23/2021 06:50:57 - INFO - datasets.builder - Generating split test\n",
            "09/23/2021 06:50:57 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 718.57it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:50:57,558 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:50:57,560 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-23 06:50:57,847 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:50:58,127 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:50:58,128 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:59,813 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:59,813 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:59,813 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:59,813 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:50:59,813 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:51:00,093 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:51:00,094 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-23 06:51:00,450 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-23 06:51:01,790 >> Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-23 06:51:01,790 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/68 [00:00<?, ?ba/s]09/23/2021 06:51:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-4666144301ebda91.arrow\n",
            "Running tokenizer on dataset: 100% 68/68 [00:04<00:00, 16.88ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/23/2021 06:51:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-097211c054395abe.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 14.19ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/23/2021 06:51:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2896df5b99669b0a.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 14.48ba/s]\n",
            "09/23/2021 06:51:06 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/23/2021 06:51:06 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/23/2021 06:51:06 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/23/2021 06:51:06 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/23/2021 06:51:06 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/23/2021 06:51:09 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-23 06:51:09,715 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: idx, sentence.\n",
            "[INFO|trainer.py:2209] 2021-09-23 06:51:09,719 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 06:51:09,719 >>   Num examples = 872\n",
            "[INFO|trainer.py:2214] 2021-09-23 06:51:09,719 >>   Batch size = 8\n",
            " 99% 108/109 [00:05<00:00, 18.43it/s]09/23/2021 06:51:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/sst2/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-23 06:51:15,647 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//sst2/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/56u6nw1e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065116-56u6nw1e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 109/109 [00:09<00:00, 11.70it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4908\n",
            "  eval_loss               =     0.7329\n",
            "  eval_runtime            = 0:00:05.91\n",
            "  eval_samples            =        872\n",
            "  eval_samples_per_second =    147.362\n",
            "  eval_steps_per_second   =      18.42\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4050\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065116-56u6nw1e/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065116-56u6nw1e/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.49083\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.73287\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 5.9174\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 147.362\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 18.42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//sst2/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/56u6nw1e\u001b[0m\n",
            "mrpc\n",
            "09/23/2021 06:51:29 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/23/2021 06:51:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//mrpc/runs/Sep23_06-51-29_bb9cb4da2be7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//mrpc/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//mrpc/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/23/2021 06:51:29 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/23/2021 06:51:29 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:51:29 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/23/2021 06:51:29 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/23/2021 06:51:29 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/23/2021 06:51:29 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:51:29 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/23/2021 06:51:29 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "  0% 0/3 [00:00<?, ?it/s]09/23/2021 06:51:29 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpvi3w9g21\n",
            "\n",
            "Downloading: 6.22kB [00:00, 4.57MB/s]\n",
            "09/23/2021 06:51:30 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv in cache at /root/.cache/huggingface/datasets/downloads/393f97e0117f7c5fc1053fba02fb070b221bc0da0eafbd6b57e8cdf7621a4e64\n",
            "09/23/2021 06:51:30 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/393f97e0117f7c5fc1053fba02fb070b221bc0da0eafbd6b57e8cdf7621a4e64\n",
            " 33% 1/3 [00:00<00:00,  2.63it/s]09/23/2021 06:51:30 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmptfrgym3w\n",
            "\n",
            "Downloading: 1.05MB [00:00, 20.2MB/s]\n",
            "09/23/2021 06:51:30 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt in cache at /root/.cache/huggingface/datasets/downloads/7c6c4f66e416181b62e136ddd5834ec10afe3aac4f7a327b81ca74025ea69529\n",
            "09/23/2021 06:51:30 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/7c6c4f66e416181b62e136ddd5834ec10afe3aac4f7a327b81ca74025ea69529\n",
            " 67% 2/3 [00:00<00:00,  3.02it/s]09/23/2021 06:51:30 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmppvrxd73b\n",
            "\n",
            "Downloading: 441kB [00:00, 9.46MB/s]\n",
            "09/23/2021 06:51:30 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt in cache at /root/.cache/huggingface/datasets/downloads/d0f75e90c732a9847ec38471fddece4ebcaad09dd1958467e2b00c6a3cbd31a9\n",
            "09/23/2021 06:51:30 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d0f75e90c732a9847ec38471fddece4ebcaad09dd1958467e2b00c6a3cbd31a9\n",
            "100% 3/3 [00:00<00:00,  3.14it/s]\n",
            "09/23/2021 06:51:30 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/23/2021 06:51:30 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/23/2021 06:51:30 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/23/2021 06:51:30 - INFO - datasets.builder - Generating split train\n",
            "09/23/2021 06:51:30 - INFO - datasets.builder - Generating split validation\n",
            "09/23/2021 06:51:31 - INFO - datasets.builder - Generating split test\n",
            "09/23/2021 06:51:31 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 501.41it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:51:31,487 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:51:31,488 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-23 06:51:31,795 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:51:32,076 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:51:32,077 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:51:33,766 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:51:33,766 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:51:33,766 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:51:33,766 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:51:33,766 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:51:34,047 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:51:34,048 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-23 06:51:34,411 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-23 06:51:35,722 >> Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-23 06:51:35,723 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]09/23/2021 06:51:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7b8752601c88d730.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00,  7.55ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/23/2021 06:51:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bfb669f7e4e4e930.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 19.14ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/23/2021 06:51:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b81567e68a47a763.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  9.56ba/s]\n",
            "09/23/2021 06:51:37 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/23/2021 06:51:37 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/23/2021 06:51:37 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/23/2021 06:51:37 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/23/2021 06:51:37 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/23/2021 06:51:40 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-23 06:51:40,261 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
            "[INFO|trainer.py:2209] 2021-09-23 06:51:40,266 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 06:51:40,266 >>   Num examples = 408\n",
            "[INFO|trainer.py:2214] 2021-09-23 06:51:40,266 >>   Batch size = 8\n",
            "100% 51/51 [00:02<00:00, 19.70it/s]09/23/2021 06:51:42 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-23 06:51:42,896 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//mrpc/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/22qwj2eg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065143-22qwj2eg\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 51/51 [00:05<00:00,  8.67it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.3554\n",
            "  eval_combined_score     =      0.278\n",
            "  eval_f1                 =     0.2006\n",
            "  eval_loss               =     0.7493\n",
            "  eval_runtime            = 0:00:02.62\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    155.704\n",
            "  eval_steps_per_second   =     19.463\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4138\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065143-22qwj2eg/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065143-22qwj2eg/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.35539\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score 0.278\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/f1 0.20061\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.74929\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 2.6204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 155.704\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 19.463\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//mrpc/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/22qwj2eg\u001b[0m\n",
            "stsb\n",
            "09/23/2021 06:51:56 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/23/2021 06:51:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//stsb/runs/Sep23_06-51-56_bb9cb4da2be7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//stsb/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//stsb/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/23/2021 06:51:56 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/23/2021 06:51:56 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:51:56 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/23/2021 06:51:56 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/23/2021 06:51:56 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/23/2021 06:51:56 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:51:56 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/stsb (download: 784.05 KiB, generated: 1.09 MiB, post-processed: Unknown size, total: 1.86 MiB) to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/23/2021 06:51:56 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/23/2021 06:51:56 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/STS-B.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpq9gb_rzi\n",
            "Downloading: 100% 803k/803k [00:00<00:00, 17.7MB/s]\n",
            "09/23/2021 06:51:57 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/STS-B.zip in cache at /root/.cache/huggingface/datasets/downloads/4950bb45e97fc693ee40800c89ad6366cb83399bbda76b6f08ec22fc3cd3d7ed\n",
            "09/23/2021 06:51:57 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/4950bb45e97fc693ee40800c89ad6366cb83399bbda76b6f08ec22fc3cd3d7ed\n",
            "09/23/2021 06:51:57 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/23/2021 06:51:57 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/23/2021 06:51:57 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/23/2021 06:51:57 - INFO - datasets.builder - Generating split train\n",
            "09/23/2021 06:51:57 - INFO - datasets.builder - Generating split validation\n",
            "09/23/2021 06:51:57 - INFO - datasets.builder - Generating split test\n",
            "09/23/2021 06:51:57 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 717.51it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:51:58,045 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:51:58,046 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"stsb\",\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-23 06:51:58,330 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:51:58,611 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:51:58,612 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:52:00,313 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:52:00,313 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:52:00,313 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:52:00,314 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:52:00,314 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:52:00,595 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:52:00,597 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-23 06:52:00,958 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-23 06:52:02,281 >> Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-23 06:52:02,281 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/6 [00:00<?, ?ba/s]09/23/2021 06:52:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c877a720a61b4df.arrow\n",
            "Running tokenizer on dataset: 100% 6/6 [00:00<00:00, 11.21ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/23/2021 06:52:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6f3122e1fed58a7c.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 14.18ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/23/2021 06:52:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-8b8421fb87ed1bf8.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 17.91ba/s]\n",
            "09/23/2021 06:52:03 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/23/2021 06:52:03 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/23/2021 06:52:03 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/23/2021 06:52:03 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/23/2021 06:52:03 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/23/2021 06:52:06 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-23 06:52:06,708 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.\n",
            "[INFO|trainer.py:2209] 2021-09-23 06:52:06,712 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 06:52:06,712 >>   Num examples = 1500\n",
            "[INFO|trainer.py:2214] 2021-09-23 06:52:06,713 >>   Batch size = 8\n",
            " 99% 187/188 [00:09<00:00, 19.89it/s]09/23/2021 06:52:16 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/stsb/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-23 06:52:16,150 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//stsb/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/1130kjxa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065216-1130kjxa\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 188/188 [00:12<00:00, 14.57it/s]\n",
            "***** eval metrics *****\n",
            "  eval_combined_score     =     0.1054\n",
            "  eval_loss               =     7.9698\n",
            "  eval_pearson            =     0.1066\n",
            "  eval_runtime            = 0:00:09.42\n",
            "  eval_samples            =       1500\n",
            "  eval_samples_per_second =    159.132\n",
            "  eval_spearmanr          =     0.1041\n",
            "  eval_steps_per_second   =     19.945\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4220\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065216-1130kjxa/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065216-1130kjxa/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score 0.10535\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 7.96982\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/pearson 0.10657\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 9.4261\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 159.132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/spearmanr 0.10413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 19.945\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/pearson ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/spearmanr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//stsb/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/1130kjxa\u001b[0m\n",
            "qqp\n",
            "09/23/2021 06:52:29 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/23/2021 06:52:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//qqp/runs/Sep23_06-52-29_bb9cb4da2be7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//qqp/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//qqp/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/23/2021 06:52:30 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/23/2021 06:52:30 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:52:30 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/23/2021 06:52:30 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/23/2021 06:52:30 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/23/2021 06:52:30 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:52:30 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/23/2021 06:52:30 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/23/2021 06:52:30 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpx3d2c60_\n",
            "Downloading: 100% 41.7M/41.7M [00:00<00:00, 42.8MB/s]\n",
            "09/23/2021 06:52:31 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip in cache at /root/.cache/huggingface/datasets/downloads/491242fd88ee341dce7c78b8aec4ec603f1de53f5227b6013aaddb563f84cd41\n",
            "09/23/2021 06:52:31 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/491242fd88ee341dce7c78b8aec4ec603f1de53f5227b6013aaddb563f84cd41\n",
            "09/23/2021 06:52:31 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/23/2021 06:52:31 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/23/2021 06:52:32 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/23/2021 06:52:32 - INFO - datasets.builder - Generating split train\n",
            "09/23/2021 06:52:51 - INFO - datasets.builder - Generating split validation\n",
            "09/23/2021 06:52:53 - INFO - datasets.builder - Generating split test\n",
            "09/23/2021 06:53:11 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 443.78it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:53:11,852 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:53:11,855 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"qqp\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-23 06:53:12,132 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:53:12,417 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:53:12,418 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:53:14,108 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:53:14,108 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:53:14,108 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:53:14,108 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:53:14,109 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:53:14,395 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:53:14,396 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-23 06:53:14,755 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-23 06:53:16,073 >> Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-23 06:53:16,073 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/364 [00:00<?, ?ba/s]09/23/2021 06:53:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-84d2d18843cfe8ba.arrow\n",
            "Running tokenizer on dataset: 100% 364/364 [00:31<00:00, 11.51ba/s]\n",
            "Running tokenizer on dataset:   0% 0/41 [00:00<?, ?ba/s]09/23/2021 06:53:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-4744aba0b945c0fa.arrow\n",
            "Running tokenizer on dataset: 100% 41/41 [00:03<00:00, 11.44ba/s]\n",
            "Running tokenizer on dataset:   0% 0/391 [00:00<?, ?ba/s]09/23/2021 06:53:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bc09447e5de5b46d.arrow\n",
            "Running tokenizer on dataset: 100% 391/391 [00:34<00:00, 11.35ba/s]\n",
            "09/23/2021 06:54:26 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/23/2021 06:54:26 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/23/2021 06:54:26 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/23/2021 06:54:26 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/23/2021 06:54:26 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/23/2021 06:54:29 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-23 06:54:29,670 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: idx, question1, question2.\n",
            "[INFO|trainer.py:2209] 2021-09-23 06:54:29,675 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 06:54:29,675 >>   Num examples = 40430\n",
            "[INFO|trainer.py:2214] 2021-09-23 06:54:29,675 >>   Batch size = 8\n",
            "100% 5053/5054 [04:12<00:00, 19.99it/s]09/23/2021 06:58:42 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/qqp/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-23 06:58:42,883 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//qqp/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/342a03n2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065843-342a03n2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 5054/5054 [04:16<00:00, 19.71it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.6294\n",
            "  eval_combined_score     =     0.3232\n",
            "  eval_f1                 =     0.0169\n",
            "  eval_loss               =     0.6563\n",
            "  eval_runtime            = 0:04:13.19\n",
            "  eval_samples            =      40430\n",
            "  eval_samples_per_second =    159.677\n",
            "  eval_steps_per_second   =     19.961\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065843-342a03n2/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_065843-342a03n2/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.62938\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score 0.32316\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/f1 0.01693\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.65626\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 253.198\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 159.677\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 19.961\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//qqp/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/342a03n2\u001b[0m\n",
            "mnli\n",
            "09/23/2021 06:58:56 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/23/2021 06:58:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//mnli/runs/Sep23_06-58-56_bb9cb4da2be7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//mnli/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//mnli/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/23/2021 06:58:56 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/23/2021 06:58:56 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:58:56 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/23/2021 06:58:56 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/23/2021 06:58:56 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/23/2021 06:58:56 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 06:58:56 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/23/2021 06:58:56 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/23/2021 06:58:57 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/MNLI.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp9oijlzk_\n",
            "Downloading: 100% 313M/313M [00:07<00:00, 39.8MB/s]\n",
            "09/23/2021 06:59:05 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/MNLI.zip in cache at /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\n",
            "09/23/2021 06:59:05 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\n",
            "09/23/2021 06:59:05 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/23/2021 06:59:06 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/23/2021 06:59:15 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/23/2021 06:59:15 - INFO - datasets.builder - Generating split train\n",
            "09/23/2021 06:59:40 - INFO - datasets.builder - Generating split validation_matched\n",
            "09/23/2021 06:59:41 - INFO - datasets.builder - Generating split validation_mismatched\n",
            "09/23/2021 06:59:42 - INFO - datasets.builder - Generating split test_matched\n",
            "09/23/2021 06:59:42 - INFO - datasets.builder - Generating split test_mismatched\n",
            "09/23/2021 06:59:43 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 5/5 [00:00<00:00, 580.98it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:59:43,642 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:59:43,644 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"mnli\",\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-23 06:59:43,922 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:59:44,203 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:59:44,204 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:59:45,884 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:59:45,884 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:59:45,884 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:59:45,884 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 06:59:45,884 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 06:59:46,161 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 06:59:46,162 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-23 06:59:46,513 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-23 06:59:47,801 >> Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-23 06:59:47,801 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/393 [00:00<?, ?ba/s]09/23/2021 06:59:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7bf16cd3fee5b8a1.arrow\n",
            "Running tokenizer on dataset: 100% 393/393 [00:42<00:00,  9.33ba/s]\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]09/23/2021 07:00:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-373a2468601ccd81.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  9.57ba/s]\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]09/23/2021 07:00:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f2a8e740eb5656a3.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  8.79ba/s]\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]09/23/2021 07:00:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-8368aeafe479d659.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  9.26ba/s]\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]09/23/2021 07:00:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-81b3a6e565ed690c.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  9.78ba/s]\n",
            "09/23/2021 07:00:35 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/23/2021 07:00:35 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/23/2021 07:00:35 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/23/2021 07:00:35 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/23/2021 07:00:35 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/23/2021 07:00:38 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-23 07:00:38,197 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
            "[INFO|trainer.py:2209] 2021-09-23 07:00:38,201 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 07:00:38,201 >>   Num examples = 9815\n",
            "[INFO|trainer.py:2214] 2021-09-23 07:00:38,201 >>   Batch size = 8\n",
            "100% 1226/1227 [01:05<00:00, 18.49it/s]09/23/2021 07:01:43 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-23 07:01:43,527 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//mnli/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/2lkgwuqx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070144-2lkgwuqx\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 1227/1227 [01:08<00:00, 17.87it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.3327\n",
            "  eval_loss               =     1.1647\n",
            "  eval_runtime            = 0:01:05.31\n",
            "  eval_samples            =       9815\n",
            "  eval_samples_per_second =    150.271\n",
            "  eval_steps_per_second   =     18.786\n",
            "[INFO|trainer.py:539] 2021-09-23 07:01:46,944 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
            "[INFO|trainer.py:2209] 2021-09-23 07:01:46,948 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 07:01:46,948 >>   Num examples = 9832\n",
            "[INFO|trainer.py:2214] 2021-09-23 07:01:46,948 >>   Batch size = 8\n",
            "100% 1228/1229 [01:05<00:00, 18.67it/s]09/23/2021 07:02:52 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\n",
            "100% 1229/1229 [01:05<00:00, 18.72it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.3353\n",
            "  eval_loss               =     1.1599\n",
            "  eval_runtime            = 0:01:05.71\n",
            "  eval_samples            =       9832\n",
            "  eval_samples_per_second =    149.615\n",
            "  eval_steps_per_second   =     18.702\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4502\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070144-2lkgwuqx/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070144-2lkgwuqx/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.33533\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 1.15991\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 65.7153\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 149.615\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 18.702\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//mnli/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/2lkgwuqx\u001b[0m\n",
            "qnli\n",
            "09/23/2021 07:03:02 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/23/2021 07:03:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//qnli/runs/Sep23_07-03-02_bb9cb4da2be7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//qnli/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//qnli/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/23/2021 07:03:02 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/23/2021 07:03:02 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 07:03:02 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/23/2021 07:03:02 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/23/2021 07:03:02 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/23/2021 07:03:02 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 07:03:02 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/qnli (download: 10.14 MiB, generated: 27.11 MiB, post-processed: Unknown size, total: 37.24 MiB) to /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/23/2021 07:03:02 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/23/2021 07:03:02 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqavulr22\n",
            "Downloading: 100% 10.6M/10.6M [00:00<00:00, 34.2MB/s]\n",
            "09/23/2021 07:03:03 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip in cache at /root/.cache/huggingface/datasets/downloads/5d46a4f1b8f53f8d373662ecca3173e141ab9c37b508b802821e9b0400f45f69\n",
            "09/23/2021 07:03:03 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5d46a4f1b8f53f8d373662ecca3173e141ab9c37b508b802821e9b0400f45f69\n",
            "09/23/2021 07:03:03 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/23/2021 07:03:03 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/23/2021 07:03:03 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/23/2021 07:03:03 - INFO - datasets.builder - Generating split train\n",
            "09/23/2021 07:03:09 - INFO - datasets.builder - Generating split validation\n",
            "09/23/2021 07:03:09 - INFO - datasets.builder - Generating split test\n",
            "09/23/2021 07:03:09 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 652.98it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 07:03:09,854 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 07:03:09,855 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"qnli\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-23 07:03:10,132 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 07:03:10,413 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 07:03:10,414 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:03:12,087 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:03:12,087 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:03:12,087 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:03:12,087 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:03:12,087 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 07:03:12,367 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 07:03:12,368 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-23 07:03:12,725 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-23 07:03:14,003 >> Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-23 07:03:14,003 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/105 [00:00<?, ?ba/s]09/23/2021 07:03:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdacc69c19170edf.arrow\n",
            "Running tokenizer on dataset: 100% 105/105 [00:13<00:00,  7.80ba/s]\n",
            "Running tokenizer on dataset:   0% 0/6 [00:00<?, ?ba/s]09/23/2021 07:03:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-02e6d7a71cefbe3a.arrow\n",
            "Running tokenizer on dataset: 100% 6/6 [00:00<00:00,  8.39ba/s]\n",
            "Running tokenizer on dataset:   0% 0/6 [00:00<?, ?ba/s]09/23/2021 07:03:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2a6843f0620e1ee0.arrow\n",
            "Running tokenizer on dataset: 100% 6/6 [00:00<00:00,  7.34ba/s]\n",
            "09/23/2021 07:03:29 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/23/2021 07:03:29 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/23/2021 07:03:29 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/23/2021 07:03:29 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/23/2021 07:03:29 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/23/2021 07:03:32 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-23 07:03:32,772 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence, question, idx.\n",
            "[INFO|trainer.py:2209] 2021-09-23 07:03:32,776 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 07:03:32,776 >>   Num examples = 5463\n",
            "[INFO|trainer.py:2214] 2021-09-23 07:03:32,777 >>   Batch size = 8\n",
            "100% 682/683 [00:36<00:00, 18.75it/s]09/23/2021 07:04:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-23 07:04:09,131 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//qnli/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/3f9jgol4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070409-3f9jgol4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 683/683 [00:39<00:00, 17.28it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4992\n",
            "  eval_loss               =     0.7395\n",
            "  eval_runtime            = 0:00:36.34\n",
            "  eval_samples            =       5463\n",
            "  eval_samples_per_second =     150.32\n",
            "  eval_steps_per_second   =     18.793\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4610\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070409-3f9jgol4/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070409-3f9jgol4/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.49918\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.73946\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 36.3425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 150.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 18.793\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//qnli/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/3f9jgol4\u001b[0m\n",
            "rte\n",
            "09/23/2021 07:04:22 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/23/2021 07:04:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//rte/runs/Sep23_07-04-22_bb9cb4da2be7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//rte/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//rte/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/23/2021 07:04:23 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/23/2021 07:04:23 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 07:04:23 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/23/2021 07:04:23 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/23/2021 07:04:23 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/23/2021 07:04:23 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 07:04:23 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/23/2021 07:04:23 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/23/2021 07:04:23 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/RTE.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp2elhs9d7\n",
            "Downloading: 100% 697k/697k [00:00<00:00, 13.0MB/s]\n",
            "09/23/2021 07:04:24 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/RTE.zip in cache at /root/.cache/huggingface/datasets/downloads/5464512a38d81a85477cab30332689dae95363efc500124519a96c5eda29226b\n",
            "09/23/2021 07:04:24 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5464512a38d81a85477cab30332689dae95363efc500124519a96c5eda29226b\n",
            "09/23/2021 07:04:24 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/23/2021 07:04:24 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/23/2021 07:04:24 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/23/2021 07:04:24 - INFO - datasets.builder - Generating split train\n",
            "09/23/2021 07:04:24 - INFO - datasets.builder - Generating split validation\n",
            "09/23/2021 07:04:24 - INFO - datasets.builder - Generating split test\n",
            "09/23/2021 07:04:24 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 687.14it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 07:04:24,783 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 07:04:24,784 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-23 07:04:25,064 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 07:04:25,344 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 07:04:25,345 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:27,041 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:27,041 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:27,041 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:27,041 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:27,041 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 07:04:27,319 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 07:04:27,320 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-23 07:04:27,678 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-23 07:04:28,992 >> Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-23 07:04:28,993 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/3 [00:00<?, ?ba/s]09/23/2021 07:04:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5443fc2f09d0aa30.arrow\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.97ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/23/2021 07:04:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-eb21cd49b1425792.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 21.50ba/s]\n",
            "Running tokenizer on dataset:   0% 0/3 [00:00<?, ?ba/s]09/23/2021 07:04:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-febe02741665a0c2.arrow\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.91ba/s]\n",
            "09/23/2021 07:04:30 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/23/2021 07:04:30 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/23/2021 07:04:30 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/23/2021 07:04:30 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/23/2021 07:04:30 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/23/2021 07:04:33 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-23 07:04:33,473 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1.\n",
            "[INFO|trainer.py:2209] 2021-09-23 07:04:33,477 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 07:04:33,477 >>   Num examples = 277\n",
            "[INFO|trainer.py:2214] 2021-09-23 07:04:33,477 >>   Batch size = 8\n",
            " 97% 34/35 [00:01<00:00, 19.88it/s]09/23/2021 07:04:35 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-23 07:04:35,288 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//rte/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/4sk6sjy0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070435-4sk6sjy0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 35/35 [00:05<00:00,  6.98it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.5126\n",
            "  eval_loss               =     0.7211\n",
            "  eval_runtime            = 0:00:01.78\n",
            "  eval_samples            =        277\n",
            "  eval_samples_per_second =    155.121\n",
            "  eval_steps_per_second   =       19.6\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4692\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070435-4sk6sjy0/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070435-4sk6sjy0/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.51264\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.72111\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 1.7857\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 155.121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 19.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//rte/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/4sk6sjy0\u001b[0m\n",
            "wnli\n",
            "09/23/2021 07:04:48 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/23/2021 07:04:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//wnli/runs/Sep23_07-04-48_bb9cb4da2be7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//wnli/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//wnli/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/23/2021 07:04:48 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/23/2021 07:04:48 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 07:04:48 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/23/2021 07:04:48 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/23/2021 07:04:48 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/23/2021 07:04:48 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/23/2021 07:04:48 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/wnli (download: 28.32 KiB, generated: 154.03 KiB, post-processed: Unknown size, total: 182.35 KiB) to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/23/2021 07:04:48 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/23/2021 07:04:48 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/WNLI.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp7lgw9x0h\n",
            "Downloading: 100% 29.0k/29.0k [00:00<00:00, 3.60MB/s]\n",
            "09/23/2021 07:04:49 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/WNLI.zip in cache at /root/.cache/huggingface/datasets/downloads/d1edd5615429cf5cdfbc0f19135ff668853953a7e0e87aceb027565666325d58\n",
            "09/23/2021 07:04:49 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d1edd5615429cf5cdfbc0f19135ff668853953a7e0e87aceb027565666325d58\n",
            "09/23/2021 07:04:49 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/23/2021 07:04:49 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/23/2021 07:04:49 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/23/2021 07:04:49 - INFO - datasets.builder - Generating split train\n",
            "09/23/2021 07:04:49 - INFO - datasets.builder - Generating split validation\n",
            "09/23/2021 07:04:49 - INFO - datasets.builder - Generating split test\n",
            "09/23/2021 07:04:49 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 757.37it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 07:04:49,469 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 07:04:49,470 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"wnli\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-23 07:04:49,764 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 07:04:50,045 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 07:04:50,046 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:51,723 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:51,723 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:51,723 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:51,723 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-23 07:04:51,723 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-23 07:04:52,004 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:611] 2021-09-23 07:04:52,005 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-23 07:04:52,364 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-23 07:04:53,747 >> Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-23 07:04:53,748 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/23/2021 07:04:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ba99800ac35bed7a.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 15.95ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/23/2021 07:04:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3dc3f428152ca646.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 125.04ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/23/2021 07:04:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9e98f43a5d4c1212.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 51.51ba/s]\n",
            "09/23/2021 07:04:54 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/23/2021 07:04:54 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/23/2021 07:04:54 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/23/2021 07:04:54 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/23/2021 07:04:54 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/23/2021 07:04:57 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-23 07:04:57,457 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.\n",
            "[INFO|trainer.py:2209] 2021-09-23 07:04:57,462 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-23 07:04:57,462 >>   Num examples = 71\n",
            "[INFO|trainer.py:2214] 2021-09-23 07:04:57,462 >>   Batch size = 8\n",
            " 78% 7/9 [00:00<00:00, 22.96it/s]09/23/2021 07:04:57 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/wnli/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-23 07:04:57,931 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//wnli/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/2xkzx70c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070458-2xkzx70c\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 9/9 [00:03<00:00,  2.41it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =      0.493\n",
            "  eval_loss               =     0.7078\n",
            "  eval_runtime            = 0:00:00.45\n",
            "  eval_samples            =         71\n",
            "  eval_samples_per_second =    154.644\n",
            "  eval_steps_per_second   =     19.603\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4774\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070458-2xkzx70c/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210923_070458-2xkzx70c/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.49296\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.7078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 0.4591\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 154.644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 19.603\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//wnli/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/2xkzx70c\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SApVGUdGTLoJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArKJa3BxihoX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z_flaDVihzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f031552-64e9-47cd-faf5-47ed77b046a7"
      },
      "source": [
        "%%shell\n",
        "for TASK_NAME in cola sst2 mrpc stsb qqp mnli qnli rte wnli\n",
        "do\n",
        "    echo $TASK_NAME\n",
        "    CUDA_VISIBLE_DEVICES=0 python run_glue.py \\\n",
        "        --model_name_or_path xlnet-large-cased \\\n",
        "        --task_name $TASK_NAME \\\n",
        "        --do_eval \\\n",
        "        --max_seq_length 128 \\\n",
        "        --per_device_train_batch_size 32 \\\n",
        "        --learning_rate 2e-5 \\\n",
        "        --num_train_epochs 3 \\\n",
        "        --output_dir saved_dir/${MODEL_NAME}/${TASK_NAME}/\n",
        "done\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cola\n",
            "09/26/2021 04:43:49 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/26/2021 04:43:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//cola/runs/Sep26_04-43-49_e1e2fa236554,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//cola/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//cola/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/26/2021 04:43:49 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpxa91s5p0\n",
            "Downloading: 28.8kB [00:00, 18.1MB/s]       \n",
            "09/26/2021 04:43:49 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/6448ad3256e939b3c47a66c9a5d8b8be11ce2f26dbb2b00f3b528bd916924204.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\n",
            "09/26/2021 04:43:49 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6448ad3256e939b3c47a66c9a5d8b8be11ce2f26dbb2b00f3b528bd916924204.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\n",
            "09/26/2021 04:43:50 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp_pmceqg8\n",
            "Downloading: 28.7kB [00:00, 22.0MB/s]       \n",
            "09/26/2021 04:43:50 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/b78f921f851167a222865867e9ff0d039acd123fb41b2b780654dc73909ed791.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\n",
            "09/26/2021 04:43:50 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b78f921f851167a222865867e9ff0d039acd123fb41b2b780654dc73909ed791.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\n",
            "09/26/2021 04:43:50 - INFO - datasets.load - Creating main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/26/2021 04:43:50 - INFO - datasets.load - Creating specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:43:50 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/26/2021 04:43:50 - INFO - datasets.load - Copying dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/26/2021 04:43:50 - INFO - datasets.load - Creating metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/26/2021 04:43:50 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:43:50 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/26/2021 04:43:50 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/26/2021 04:43:50 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/CoLA.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpfi5t1s4p\n",
            "Downloading: 100% 377k/377k [00:00<00:00, 1.17MB/s]\n",
            "09/26/2021 04:43:51 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/CoLA.zip in cache at /root/.cache/huggingface/datasets/downloads/bb971ae26c644d1ca0a93f2edb4402d5934802431d3ccce209d74ddeef0c5815\n",
            "09/26/2021 04:43:51 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/bb971ae26c644d1ca0a93f2edb4402d5934802431d3ccce209d74ddeef0c5815\n",
            "09/26/2021 04:43:51 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/26/2021 04:43:51 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/26/2021 04:43:51 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/26/2021 04:43:51 - INFO - datasets.builder - Generating split train\n",
            "09/26/2021 04:43:51 - INFO - datasets.builder - Generating split validation\n",
            "09/26/2021 04:43:51 - INFO - datasets.builder - Generating split test\n",
            "09/26/2021 04:43:51 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 375.70it/s]\n",
            "[INFO|file_utils.py:1664] 2021-09-26 04:43:52,117 >> https://huggingface.co/xlnet-large-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpaoslgnvm\n",
            "Downloading: 100% 761/761 [00:00<00:00, 998kB/s]\n",
            "[INFO|file_utils.py:1668] 2021-09-26 04:43:52,239 >> storing https://huggingface.co/xlnet-large-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|file_utils.py:1676] 2021-09-26 04:43:52,239 >> creating metadata file for /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:43:52,239 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:43:52,241 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-26 04:43:52,361 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:43:52,484 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:43:52,485 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1664] 2021-09-26 04:43:52,735 >> https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdqrrad2t\n",
            "Downloading: 100% 779k/779k [00:00<00:00, 5.01MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-09-26 04:43:53,023 >> storing https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|file_utils.py:1676] 2021-09-26 04:43:53,023 >> creating metadata file for /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|file_utils.py:1664] 2021-09-26 04:43:53,147 >> https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpde3wyhtn\n",
            "Downloading: 100% 1.32M/1.32M [00:00<00:00, 8.00MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-09-26 04:43:53,452 >> storing https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|file_utils.py:1676] 2021-09-26 04:43:53,452 >> creating metadata file for /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:43:53,817 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:43:53,817 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:43:53,817 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:43:53,817 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:43:53,817 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:43:53,939 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:43:53,940 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1664] 2021-09-26 04:43:54,138 >> https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_el_t05s\n",
            "Downloading: 100% 1.34G/1.34G [00:34<00:00, 42.0MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-09-26 04:44:28,861 >> storing https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[INFO|file_utils.py:1676] 2021-09-26 04:44:28,861 >> creating metadata file for /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[INFO|modeling_utils.py:1291] 2021-09-26 04:44:28,861 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-26 04:44:32,927 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-26 04:44:32,927 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/9 [00:00<?, ?ba/s]09/26/2021 04:44:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ad9fe74566b0d07e.arrow\n",
            "Running tokenizer on dataset: 100% 9/9 [00:00<00:00, 15.39ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/26/2021 04:44:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-82ec791567939cd5.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 30.39ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/26/2021 04:44:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b62a4ed55bc3e3aa.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 32.38ba/s]\n",
            "09/26/2021 04:44:33 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmptr0ibsu1\n",
            "Downloading: 5.78kB [00:00, 6.31MB/s]       \n",
            "09/26/2021 04:44:34 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/3d00b264fb94fc8d9977c2149d05fc9eb1f8a5e485aadb2eae351dfc9c3a6db4.d4934ed99ee0ef4a5d0d757367c52f72d6ae30a68db09205c01563cab11c5a0d.py\n",
            "09/26/2021 04:44:34 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3d00b264fb94fc8d9977c2149d05fc9eb1f8a5e485aadb2eae351dfc9c3a6db4.d4934ed99ee0ef4a5d0d757367c52f72d6ae30a68db09205c01563cab11c5a0d.py\n",
            "09/26/2021 04:44:34 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/26/2021 04:44:34 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/26/2021 04:44:34 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/26/2021 04:44:34 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/26/2021 04:44:34 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/26/2021 04:44:41 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-26 04:44:41,729 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence, idx.\n",
            "[INFO|trainer.py:2209] 2021-09-26 04:44:41,733 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 04:44:41,733 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2214] 2021-09-26 04:44:41,733 >>   Batch size = 8\n",
            " 99% 130/131 [00:21<00:00,  6.13it/s]09/26/2021 04:45:03 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-26 04:45:03,154 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//cola/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/368cks3i\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044503-368cks3i\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 131/131 [00:24<00:00,  5.34it/s]\n",
            "***** eval metrics *****\n",
            "  eval_loss                 =     0.6762\n",
            "  eval_matthews_correlation =    -0.0335\n",
            "  eval_runtime              = 0:00:21.40\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =      48.72\n",
            "  eval_steps_per_second     =      6.119\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1068\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044503-368cks3i/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044503-368cks3i/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/loss 0.67621\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/matthews_correlation -0.03349\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/runtime 21.4079\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/samples_per_second 48.72\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/steps_per_second 6.119\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/matthews_correlation ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//cola/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/368cks3i\u001b[0m\n",
            "sst2\n",
            "09/26/2021 04:45:16 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/26/2021 04:45:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//sst2/runs/Sep26_04-45-16_e1e2fa236554,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//sst2/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//sst2/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/26/2021 04:45:16 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/26/2021 04:45:16 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:45:16 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/26/2021 04:45:16 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/26/2021 04:45:16 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/26/2021 04:45:16 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:45:16 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/26/2021 04:45:17 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/26/2021 04:45:17 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/SST-2.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp3_td08pt\n",
            "Downloading: 100% 7.44M/7.44M [00:00<00:00, 9.73MB/s]\n",
            "09/26/2021 04:45:18 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/SST-2.zip in cache at /root/.cache/huggingface/datasets/downloads/fda31747115223e6a71a9b791b0723e7b113a7441b9ebc2eacd14d31e2a855ba\n",
            "09/26/2021 04:45:18 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/fda31747115223e6a71a9b791b0723e7b113a7441b9ebc2eacd14d31e2a855ba\n",
            "09/26/2021 04:45:18 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/26/2021 04:45:18 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/26/2021 04:45:18 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/26/2021 04:45:18 - INFO - datasets.builder - Generating split train\n",
            "09/26/2021 04:45:21 - INFO - datasets.builder - Generating split validation\n",
            "09/26/2021 04:45:21 - INFO - datasets.builder - Generating split test\n",
            "09/26/2021 04:45:21 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 711.30it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:45:21,747 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:45:21,749 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-26 04:45:21,869 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:45:21,990 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:45:21,991 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:45:22,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:45:22,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:45:22,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:45:22,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:45:22,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:45:22,841 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:45:22,842 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-26 04:45:23,039 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-26 04:45:27,032 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-26 04:45:27,032 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/68 [00:00<?, ?ba/s]09/26/2021 04:45:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-edeb9c0e7983d260.arrow\n",
            "Running tokenizer on dataset: 100% 68/68 [00:04<00:00, 15.52ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/26/2021 04:45:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-94ad799b479efd9d.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 14.54ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/26/2021 04:45:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-0292dd40fe5e7972.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 13.66ba/s]\n",
            "09/26/2021 04:45:31 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/26/2021 04:45:31 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/26/2021 04:45:31 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/26/2021 04:45:31 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/26/2021 04:45:31 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/26/2021 04:45:36 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-26 04:45:36,233 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: idx, sentence.\n",
            "[INFO|trainer.py:2209] 2021-09-26 04:45:36,237 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 04:45:36,237 >>   Num examples = 872\n",
            "[INFO|trainer.py:2214] 2021-09-26 04:45:36,237 >>   Batch size = 8\n",
            "100% 109/109 [00:17<00:00,  6.14it/s]09/26/2021 04:45:53 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/sst2/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-26 04:45:54,007 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//sst2/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/12ln35nx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044554-12ln35nx\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 109/109 [00:21<00:00,  5.18it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4885\n",
            "  eval_loss               =       0.71\n",
            "  eval_runtime            = 0:00:17.76\n",
            "  eval_samples            =        872\n",
            "  eval_samples_per_second =     49.095\n",
            "  eval_steps_per_second   =      6.137\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1160\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044554-12ln35nx/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044554-12ln35nx/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.48853\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.71001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 17.7616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 49.095\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 6.137\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//sst2/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/12ln35nx\u001b[0m\n",
            "mrpc\n",
            "09/26/2021 04:46:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/26/2021 04:46:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//mrpc/runs/Sep26_04-46-07_e1e2fa236554,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//mrpc/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//mrpc/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/26/2021 04:46:07 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/26/2021 04:46:07 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:46:07 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/26/2021 04:46:07 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/26/2021 04:46:07 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/26/2021 04:46:07 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:46:07 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/26/2021 04:46:07 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "  0% 0/3 [00:00<?, ?it/s]09/26/2021 04:46:08 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp4z40pm4a\n",
            "\n",
            "Downloading: 6.22kB [00:00, 3.57MB/s]\n",
            "09/26/2021 04:46:08 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv in cache at /root/.cache/huggingface/datasets/downloads/393f97e0117f7c5fc1053fba02fb070b221bc0da0eafbd6b57e8cdf7621a4e64\n",
            "09/26/2021 04:46:08 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/393f97e0117f7c5fc1053fba02fb070b221bc0da0eafbd6b57e8cdf7621a4e64\n",
            " 33% 1/3 [00:00<00:01,  1.14it/s]09/26/2021 04:46:09 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp0lrkqpva\n",
            "\n",
            "Downloading: 0.00B [00:00, ?B/s]\u001b[A\n",
            "Downloading: 54.4kB [00:00, 307kB/s]\u001b[A\n",
            "Downloading: 124kB [00:00, 473kB/s] \u001b[A\n",
            "Downloading: 1.05MB [00:00, 2.23MB/s]\n",
            "09/26/2021 04:46:10 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt in cache at /root/.cache/huggingface/datasets/downloads/7c6c4f66e416181b62e136ddd5834ec10afe3aac4f7a327b81ca74025ea69529\n",
            "09/26/2021 04:46:10 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/7c6c4f66e416181b62e136ddd5834ec10afe3aac4f7a327b81ca74025ea69529\n",
            " 67% 2/3 [00:02<00:01,  1.19s/it]09/26/2021 04:46:10 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp_77_4a79\n",
            "\n",
            "Downloading: 0.00B [00:00, ?B/s]\u001b[A\n",
            "Downloading: 45.2kB [00:00, 273kB/s]\u001b[A\n",
            "Downloading: 441kB [00:00, 1.24MB/s]\n",
            "09/26/2021 04:46:11 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt in cache at /root/.cache/huggingface/datasets/downloads/d0f75e90c732a9847ec38471fddece4ebcaad09dd1958467e2b00c6a3cbd31a9\n",
            "09/26/2021 04:46:11 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d0f75e90c732a9847ec38471fddece4ebcaad09dd1958467e2b00c6a3cbd31a9\n",
            "100% 3/3 [00:03<00:00,  1.16s/it]\n",
            "09/26/2021 04:46:11 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/26/2021 04:46:11 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/26/2021 04:46:11 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/26/2021 04:46:11 - INFO - datasets.builder - Generating split train\n",
            "09/26/2021 04:46:11 - INFO - datasets.builder - Generating split validation\n",
            "09/26/2021 04:46:11 - INFO - datasets.builder - Generating split test\n",
            "09/26/2021 04:46:12 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 676.14it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:46:12,249 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:46:12,250 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-26 04:46:12,372 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:46:12,494 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:46:12,495 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:13,227 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:13,227 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:13,227 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:13,228 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:13,228 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:46:13,348 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:46:13,349 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-26 04:46:13,547 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-26 04:46:17,453 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-26 04:46:17,453 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]09/26/2021 04:46:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-80e209d1b5f00102.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00,  6.55ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/26/2021 04:46:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-74c7a0670054f9b6.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 19.22ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/26/2021 04:46:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-557a5cd7319d5932.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  9.77ba/s]\n",
            "09/26/2021 04:46:18 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/26/2021 04:46:18 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/26/2021 04:46:18 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/26/2021 04:46:18 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/26/2021 04:46:18 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/26/2021 04:46:22 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-26 04:46:22,298 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
            "[INFO|trainer.py:2209] 2021-09-26 04:46:22,302 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 04:46:22,302 >>   Num examples = 408\n",
            "[INFO|trainer.py:2214] 2021-09-26 04:46:22,302 >>   Batch size = 8\n",
            "100% 51/51 [00:07<00:00,  6.35it/s]09/26/2021 04:46:30 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-26 04:46:30,359 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//mrpc/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/tqzj1hje\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044630-tqzj1hje\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 51/51 [00:11<00:00,  4.55it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.5147\n",
            "  eval_combined_score     =     0.5806\n",
            "  eval_f1                 =     0.6464\n",
            "  eval_loss               =     0.7025\n",
            "  eval_runtime            = 0:00:08.04\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =     50.694\n",
            "  eval_steps_per_second   =      6.337\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044630-tqzj1hje/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044630-tqzj1hje/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.51471\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score 0.58057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/f1 0.64643\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.70245\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 8.0483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 50.694\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 6.337\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//mrpc/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/tqzj1hje\u001b[0m\n",
            "stsb\n",
            "09/26/2021 04:46:43 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/26/2021 04:46:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//stsb/runs/Sep26_04-46-43_e1e2fa236554,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//stsb/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//stsb/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/26/2021 04:46:44 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/26/2021 04:46:44 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:46:44 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/26/2021 04:46:44 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/26/2021 04:46:44 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/26/2021 04:46:44 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:46:44 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/stsb (download: 784.05 KiB, generated: 1.09 MiB, post-processed: Unknown size, total: 1.86 MiB) to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/26/2021 04:46:44 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/26/2021 04:46:44 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/STS-B.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpjyd2qjci\n",
            "Downloading: 100% 803k/803k [00:00<00:00, 1.79MB/s]\n",
            "09/26/2021 04:46:45 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/STS-B.zip in cache at /root/.cache/huggingface/datasets/downloads/4950bb45e97fc693ee40800c89ad6366cb83399bbda76b6f08ec22fc3cd3d7ed\n",
            "09/26/2021 04:46:45 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/4950bb45e97fc693ee40800c89ad6366cb83399bbda76b6f08ec22fc3cd3d7ed\n",
            "09/26/2021 04:46:45 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/26/2021 04:46:45 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/26/2021 04:46:45 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/26/2021 04:46:45 - INFO - datasets.builder - Generating split train\n",
            "09/26/2021 04:46:46 - INFO - datasets.builder - Generating split validation\n",
            "09/26/2021 04:46:46 - INFO - datasets.builder - Generating split test\n",
            "09/26/2021 04:46:46 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 682.19it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:46:46,325 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:46:46,327 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"stsb\",\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-26 04:46:46,448 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:46:46,569 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:46:46,570 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:47,297 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:47,297 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:47,297 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:47,297 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:46:47,297 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:46:47,418 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:46:47,419 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-26 04:46:47,617 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-26 04:46:51,572 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-26 04:46:51,572 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/6 [00:00<?, ?ba/s]09/26/2021 04:46:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-4159a508f2e5b007.arrow\n",
            "Running tokenizer on dataset: 100% 6/6 [00:00<00:00,  9.42ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/26/2021 04:46:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3545aaf115d22a88.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 15.23ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]09/26/2021 04:46:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-58eb5a0faade9c1a.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 15.69ba/s]\n",
            "09/26/2021 04:46:52 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/26/2021 04:46:52 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/26/2021 04:46:52 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/26/2021 04:46:52 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/26/2021 04:46:52 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/26/2021 04:46:56 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-26 04:46:56,496 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2.\n",
            "[INFO|trainer.py:2209] 2021-09-26 04:46:56,500 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 04:46:56,500 >>   Num examples = 1500\n",
            "[INFO|trainer.py:2214] 2021-09-26 04:46:56,500 >>   Batch size = 8\n",
            " 99% 187/188 [00:29<00:00,  6.31it/s]09/26/2021 04:47:26 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/stsb/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-26 04:47:26,329 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//stsb/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/2qq79l5f\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044726-2qq79l5f\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 188/188 [00:32<00:00,  5.70it/s]\n",
            "***** eval metrics *****\n",
            "  eval_combined_score     =     0.0145\n",
            "  eval_loss               =      9.784\n",
            "  eval_pearson            =     0.0149\n",
            "  eval_runtime            = 0:00:29.80\n",
            "  eval_samples            =       1500\n",
            "  eval_samples_per_second =     50.327\n",
            "  eval_spearmanr          =     0.0141\n",
            "  eval_steps_per_second   =      6.308\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1334\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044726-2qq79l5f/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_044726-2qq79l5f/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score 0.01451\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 9.78402\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/pearson 0.01494\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 29.8051\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 50.327\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/spearmanr 0.01409\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 6.308\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/pearson ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/spearmanr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//stsb/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/2qq79l5f\u001b[0m\n",
            "qqp\n",
            "09/26/2021 04:47:39 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/26/2021 04:47:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//qqp/runs/Sep26_04-47-39_e1e2fa236554,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//qqp/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//qqp/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/26/2021 04:47:39 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/26/2021 04:47:39 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:47:39 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/26/2021 04:47:39 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/26/2021 04:47:39 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/26/2021 04:47:39 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 04:47:39 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/26/2021 04:47:40 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/26/2021 04:47:40 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpiv4gzzvr\n",
            "Downloading: 100% 41.7M/41.7M [00:02<00:00, 18.1MB/s]\n",
            "09/26/2021 04:47:43 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip in cache at /root/.cache/huggingface/datasets/downloads/491242fd88ee341dce7c78b8aec4ec603f1de53f5227b6013aaddb563f84cd41\n",
            "09/26/2021 04:47:43 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/491242fd88ee341dce7c78b8aec4ec603f1de53f5227b6013aaddb563f84cd41\n",
            "09/26/2021 04:47:43 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/26/2021 04:47:43 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/26/2021 04:47:44 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/26/2021 04:47:44 - INFO - datasets.builder - Generating split train\n",
            "09/26/2021 04:48:02 - INFO - datasets.builder - Generating split validation\n",
            "09/26/2021 04:48:04 - INFO - datasets.builder - Generating split test\n",
            "09/26/2021 04:48:21 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 448.17it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:48:21,794 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:48:21,796 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"qqp\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-26 04:48:21,918 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:48:22,041 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:48:22,042 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:48:22,781 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:48:22,782 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:48:22,782 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:48:22,782 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 04:48:22,782 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 04:48:22,905 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 04:48:22,906 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-26 04:48:23,105 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-26 04:48:27,018 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-26 04:48:27,018 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/364 [00:00<?, ?ba/s]09/26/2021 04:48:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-79e453a2701593a9.arrow\n",
            "Running tokenizer on dataset: 100% 364/364 [00:32<00:00, 11.26ba/s]\n",
            "Running tokenizer on dataset:   0% 0/41 [00:00<?, ?ba/s]09/26/2021 04:48:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a1aa7099534d2de5.arrow\n",
            "Running tokenizer on dataset: 100% 41/41 [00:03<00:00, 11.08ba/s]\n",
            "Running tokenizer on dataset:   0% 0/391 [00:00<?, ?ba/s]09/26/2021 04:49:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5b259d9cfe90933a.arrow\n",
            "Running tokenizer on dataset: 100% 391/391 [00:34<00:00, 11.37ba/s]\n",
            "09/26/2021 04:49:37 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/26/2021 04:49:37 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/26/2021 04:49:37 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/26/2021 04:49:37 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/26/2021 04:49:37 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/26/2021 04:49:42 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-26 04:49:42,235 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: idx, question2, question1.\n",
            "[INFO|trainer.py:2209] 2021-09-26 04:49:42,239 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 04:49:42,239 >>   Num examples = 40430\n",
            "[INFO|trainer.py:2214] 2021-09-26 04:49:42,239 >>   Batch size = 8\n",
            "100% 5054/5054 [13:40<00:00,  6.63it/s]09/26/2021 05:03:22 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/qqp/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-26 05:03:22,802 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//qqp/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/m361wpmv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_050323-m361wpmv\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 5054/5054 [13:43<00:00,  6.14it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4019\n",
            "  eval_combined_score     =     0.4129\n",
            "  eval_f1                 =     0.4239\n",
            "  eval_loss               =     0.7326\n",
            "  eval_runtime            = 0:13:40.55\n",
            "  eval_samples            =      40430\n",
            "  eval_samples_per_second =     49.272\n",
            "  eval_steps_per_second   =      6.159\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1628\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_050323-m361wpmv/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_050323-m361wpmv/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.4019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score 0.4129\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/f1 0.42389\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.73264\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 820.5535\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 49.272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 6.159\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/combined_score ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//qqp/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/m361wpmv\u001b[0m\n",
            "mnli\n",
            "09/26/2021 05:03:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/26/2021 05:03:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//mnli/runs/Sep26_05-03-36_e1e2fa236554,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//mnli/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//mnli/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/26/2021 05:03:36 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/26/2021 05:03:36 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 05:03:36 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/26/2021 05:03:36 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/26/2021 05:03:36 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/26/2021 05:03:36 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 05:03:36 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/26/2021 05:03:36 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/26/2021 05:03:37 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/MNLI.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpvxlslhkr\n",
            "Downloading: 100% 313M/313M [00:15<00:00, 20.4MB/s]\n",
            "09/26/2021 05:03:53 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/MNLI.zip in cache at /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\n",
            "09/26/2021 05:03:53 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\n",
            "09/26/2021 05:03:53 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/26/2021 05:03:54 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/26/2021 05:04:03 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/26/2021 05:04:03 - INFO - datasets.builder - Generating split train\n",
            "09/26/2021 05:04:29 - INFO - datasets.builder - Generating split validation_matched\n",
            "09/26/2021 05:04:30 - INFO - datasets.builder - Generating split validation_mismatched\n",
            "09/26/2021 05:04:31 - INFO - datasets.builder - Generating split test_matched\n",
            "09/26/2021 05:04:31 - INFO - datasets.builder - Generating split test_mismatched\n",
            "09/26/2021 05:04:32 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 5/5 [00:00<00:00, 615.69it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:04:32,669 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:04:32,670 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"mnli\",\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-26 05:04:32,790 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:04:32,911 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:04:32,912 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:04:33,647 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:04:33,647 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:04:33,647 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:04:33,647 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:04:33,647 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:04:33,770 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:04:33,771 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-26 05:04:33,968 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-26 05:04:37,866 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-26 05:04:37,866 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/393 [00:00<?, ?ba/s]09/26/2021 05:04:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e34fb01786f7db9c.arrow\n",
            "Running tokenizer on dataset: 100% 393/393 [00:41<00:00,  9.36ba/s]\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]09/26/2021 05:05:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-457eb2a24dbeb9ef.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:00<00:00, 10.15ba/s]\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]09/26/2021 05:05:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-98d3977624ad9087.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  8.95ba/s]\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]09/26/2021 05:05:22 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cf7b0cbe0a7efba7.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  9.19ba/s]\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]09/26/2021 05:05:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3da24c57b1dc8a1f.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  9.80ba/s]\n",
            "09/26/2021 05:05:24 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/26/2021 05:05:24 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/26/2021 05:05:24 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/26/2021 05:05:24 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/26/2021 05:05:24 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/26/2021 05:05:28 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-26 05:05:28,700 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\n",
            "[INFO|trainer.py:2209] 2021-09-26 05:05:28,704 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 05:05:28,704 >>   Num examples = 9815\n",
            "[INFO|trainer.py:2214] 2021-09-26 05:05:28,704 >>   Batch size = 8\n",
            "100% 1227/1227 [03:19<00:00,  6.56it/s]09/26/2021 05:08:48 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-26 05:08:48,334 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//mnli/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/1ilaik1a\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_050848-1ilaik1a\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 1227/1227 [03:22<00:00,  6.05it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.3458\n",
            "  eval_loss               =     1.1091\n",
            "  eval_runtime            = 0:03:19.62\n",
            "  eval_samples            =       9815\n",
            "  eval_samples_per_second =     49.168\n",
            "  eval_steps_per_second   =      6.147\n",
            "[INFO|trainer.py:539] 2021-09-26 05:08:51,811 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\n",
            "[INFO|trainer.py:2209] 2021-09-26 05:08:51,817 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 05:08:51,817 >>   Num examples = 9832\n",
            "[INFO|trainer.py:2214] 2021-09-26 05:08:51,818 >>   Batch size = 8\n",
            "100% 1229/1229 [03:20<00:00,  6.15it/s]09/26/2021 05:12:12 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\n",
            "100% 1229/1229 [03:20<00:00,  6.13it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.3393\n",
            "  eval_loss               =     1.1111\n",
            "  eval_runtime            = 0:03:20.58\n",
            "  eval_samples            =       9832\n",
            "  eval_samples_per_second =     49.017\n",
            "  eval_steps_per_second   =      6.127\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_050848-1ilaik1a/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_050848-1ilaik1a/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.3393\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 1.11105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 200.5832\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 49.017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 6.127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//mnli/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/1ilaik1a\u001b[0m\n",
            "qnli\n",
            "09/26/2021 05:12:22 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/26/2021 05:12:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//qnli/runs/Sep26_05-12-22_e1e2fa236554,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//qnli/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//qnli/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/26/2021 05:12:23 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/26/2021 05:12:23 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 05:12:23 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/26/2021 05:12:23 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/26/2021 05:12:23 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/26/2021 05:12:23 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 05:12:23 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/qnli (download: 10.14 MiB, generated: 27.11 MiB, post-processed: Unknown size, total: 37.24 MiB) to /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/26/2021 05:12:23 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/26/2021 05:12:23 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8zuohs9x\n",
            "Downloading: 100% 10.6M/10.6M [00:00<00:00, 11.6MB/s]\n",
            "09/26/2021 05:12:25 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip in cache at /root/.cache/huggingface/datasets/downloads/5d46a4f1b8f53f8d373662ecca3173e141ab9c37b508b802821e9b0400f45f69\n",
            "09/26/2021 05:12:25 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5d46a4f1b8f53f8d373662ecca3173e141ab9c37b508b802821e9b0400f45f69\n",
            "09/26/2021 05:12:25 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/26/2021 05:12:25 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/26/2021 05:12:25 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/26/2021 05:12:25 - INFO - datasets.builder - Generating split train\n",
            "09/26/2021 05:12:31 - INFO - datasets.builder - Generating split validation\n",
            "09/26/2021 05:12:31 - INFO - datasets.builder - Generating split test\n",
            "09/26/2021 05:12:31 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 667.49it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:12:31,690 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:12:31,691 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"qnli\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-26 05:12:31,816 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:12:31,938 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:12:31,939 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:12:32,677 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:12:32,677 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:12:32,677 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:12:32,677 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:12:32,677 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:12:32,799 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:12:32,799 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-26 05:12:32,998 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-26 05:12:36,959 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-26 05:12:36,959 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/105 [00:00<?, ?ba/s]09/26/2021 05:12:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d5ef0e306e3fc54d.arrow\n",
            "Running tokenizer on dataset: 100% 105/105 [00:13<00:00,  7.61ba/s]\n",
            "Running tokenizer on dataset:   0% 0/6 [00:00<?, ?ba/s]09/26/2021 05:12:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-71f0488c88466399.arrow\n",
            "Running tokenizer on dataset: 100% 6/6 [00:00<00:00,  8.58ba/s]\n",
            "Running tokenizer on dataset:   0% 0/6 [00:00<?, ?ba/s]09/26/2021 05:12:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-4e8671ed5f70cbe2.arrow\n",
            "Running tokenizer on dataset: 100% 6/6 [00:00<00:00,  8.48ba/s]\n",
            "09/26/2021 05:12:52 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/26/2021 05:12:52 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/26/2021 05:12:52 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/26/2021 05:12:52 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/26/2021 05:12:52 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/26/2021 05:12:57 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-26 05:12:57,014 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence, question, idx.\n",
            "[INFO|trainer.py:2209] 2021-09-26 05:12:57,017 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 05:12:57,017 >>   Num examples = 5463\n",
            "[INFO|trainer.py:2214] 2021-09-26 05:12:57,017 >>   Batch size = 8\n",
            "100% 683/683 [01:50<00:00,  6.59it/s]09/26/2021 05:14:47 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-26 05:14:47,994 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//qnli/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/255sy8hv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_051448-255sy8hv\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 683/683 [01:54<00:00,  5.99it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.5286\n",
            "  eval_loss               =     0.6976\n",
            "  eval_runtime            = 0:01:50.96\n",
            "  eval_samples            =       5463\n",
            "  eval_samples_per_second =      49.23\n",
            "  eval_steps_per_second   =      6.155\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1946\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_051448-255sy8hv/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_051448-255sy8hv/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.52865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.69761\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 110.9687\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 49.23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 6.155\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//qnli/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/255sy8hv\u001b[0m\n",
            "rte\n",
            "09/26/2021 05:15:01 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/26/2021 05:15:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//rte/runs/Sep26_05-15-01_e1e2fa236554,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//rte/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//rte/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/26/2021 05:15:01 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/26/2021 05:15:01 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 05:15:01 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/26/2021 05:15:01 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/26/2021 05:15:01 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/26/2021 05:15:01 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 05:15:01 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/26/2021 05:15:01 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/26/2021 05:15:02 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/RTE.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpejlkibgy\n",
            "Downloading: 100% 697k/697k [00:00<00:00, 1.71MB/s]\n",
            "09/26/2021 05:15:03 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/RTE.zip in cache at /root/.cache/huggingface/datasets/downloads/5464512a38d81a85477cab30332689dae95363efc500124519a96c5eda29226b\n",
            "09/26/2021 05:15:03 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5464512a38d81a85477cab30332689dae95363efc500124519a96c5eda29226b\n",
            "09/26/2021 05:15:03 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/26/2021 05:15:03 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/26/2021 05:15:03 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/26/2021 05:15:03 - INFO - datasets.builder - Generating split train\n",
            "09/26/2021 05:15:03 - INFO - datasets.builder - Generating split validation\n",
            "09/26/2021 05:15:03 - INFO - datasets.builder - Generating split test\n",
            "09/26/2021 05:15:03 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 654.95it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:15:03,689 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:15:03,690 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-26 05:15:03,811 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:15:03,933 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:15:03,934 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:04,668 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:04,668 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:04,668 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:04,668 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:04,668 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:15:04,789 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:15:04,790 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-26 05:15:04,984 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-26 05:15:08,933 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-26 05:15:08,933 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/3 [00:00<?, ?ba/s]09/26/2021 05:15:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a307918c9b0026d2.arrow\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  5.44ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/26/2021 05:15:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5787eb8e0ea2a8c9.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 20.99ba/s]\n",
            "Running tokenizer on dataset:   0% 0/3 [00:00<?, ?ba/s]09/26/2021 05:15:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ce92cf7990170c4e.arrow\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.69ba/s]\n",
            "09/26/2021 05:15:10 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/26/2021 05:15:10 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/26/2021 05:15:10 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/26/2021 05:15:10 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/26/2021 05:15:10 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/26/2021 05:15:14 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-26 05:15:14,006 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.\n",
            "[INFO|trainer.py:2209] 2021-09-26 05:15:14,009 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 05:15:14,009 >>   Num examples = 277\n",
            "[INFO|trainer.py:2214] 2021-09-26 05:15:14,010 >>   Batch size = 8\n",
            "100% 35/35 [00:05<00:00,  6.98it/s]09/26/2021 05:15:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-26 05:15:19,656 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//rte/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/1tg9axty\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_051520-1tg9axty\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 35/35 [00:08<00:00,  3.99it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4693\n",
            "  eval_loss               =     0.7245\n",
            "  eval_runtime            = 0:00:05.63\n",
            "  eval_samples            =        277\n",
            "  eval_samples_per_second =     49.127\n",
            "  eval_steps_per_second   =      6.207\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2030\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_051520-1tg9axty/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_051520-1tg9axty/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.46931\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.72448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 5.6384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 49.127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 6.207\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//rte/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/1tg9axty\u001b[0m\n",
            "wnli\n",
            "09/26/2021 05:15:33 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/26/2021 05:15:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=None,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved_dir//wnli/runs/Sep26_05-15-32_e1e2fa236554,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=saved_dir//wnli/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saved_dir//wnli/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/26/2021 05:15:33 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "09/26/2021 05:15:33 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 05:15:33 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "09/26/2021 05:15:33 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "09/26/2021 05:15:33 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "09/26/2021 05:15:33 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "09/26/2021 05:15:33 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/wnli (download: 28.32 KiB, generated: 154.03 KiB, post-processed: Unknown size, total: 182.35 KiB) to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "09/26/2021 05:15:33 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "09/26/2021 05:15:33 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/WNLI.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpq_pccolf\n",
            "Downloading: 100% 29.0k/29.0k [00:00<00:00, 330kB/s]\n",
            "09/26/2021 05:15:34 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/WNLI.zip in cache at /root/.cache/huggingface/datasets/downloads/d1edd5615429cf5cdfbc0f19135ff668853953a7e0e87aceb027565666325d58\n",
            "09/26/2021 05:15:34 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d1edd5615429cf5cdfbc0f19135ff668853953a7e0e87aceb027565666325d58\n",
            "09/26/2021 05:15:34 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "09/26/2021 05:15:34 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "09/26/2021 05:15:34 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "09/26/2021 05:15:34 - INFO - datasets.builder - Generating split train\n",
            "09/26/2021 05:15:34 - INFO - datasets.builder - Generating split validation\n",
            "09/26/2021 05:15:34 - INFO - datasets.builder - Generating split test\n",
            "09/26/2021 05:15:34 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 568.18it/s]\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:15:34,509 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:15:34,511 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"wnli\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-09-26 05:15:34,715 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:15:34,837 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:15:34,838 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:35,576 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:35,576 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:35,576 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:35,577 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-26 05:15:35,577 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:574] 2021-09-26 05:15:35,702 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:611] 2021-09-26 05:15:35,703 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1291] 2021-09-26 05:15:35,903 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1539] 2021-09-26 05:15:39,957 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1550] 2021-09-26 05:15:39,957 >> Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/26/2021 05:15:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9fb8e4fc293cf5ba.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  2.20ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/26/2021 05:15:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-40a7a0d4dc1b9a6a.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 107.42ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]09/26/2021 05:15:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-229e2e9b0c2bc3df.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 49.93ba/s]\n",
            "09/26/2021 05:15:40 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "09/26/2021 05:15:40 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "09/26/2021 05:15:40 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "09/26/2021 05:15:40 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "09/26/2021 05:15:40 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "09/26/2021 05:15:45 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:539] 2021-09-26 05:15:45,004 >> The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
            "[INFO|trainer.py:2209] 2021-09-26 05:15:45,007 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2211] 2021-09-26 05:15:45,007 >>   Num examples = 71\n",
            "[INFO|trainer.py:2214] 2021-09-26 05:15:45,007 >>   Batch size = 8\n",
            "100% 9/9 [00:01<00:00,  7.00it/s]09/26/2021 05:15:46 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/wnli/default_experiment-1-0.arrow\n",
            "[INFO|integrations.py:448] 2021-09-26 05:15:46,429 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maabayomi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaved_dir//wnli/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aabayomi/huggingface/runs/1v4pv6el\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_051546-1v4pv6el\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 9/9 [00:04<00:00,  1.98it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4225\n",
            "  eval_loss               =       0.76\n",
            "  eval_runtime            = 0:00:01.41\n",
            "  eval_samples            =         71\n",
            "  eval_samples_per_second =     50.239\n",
            "  eval_steps_per_second   =      6.368\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_051546-1v4pv6el/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/gdrive/My Drive/deep learning/transformers/examples/pytorch/text-classification/wandb/run-20210926_051546-1v4pv6el/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy 0.42254\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss 0.75999\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime 1.4132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second 50.239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second 6.368\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msaved_dir//wnli/\u001b[0m: \u001b[34mhttps://wandb.ai/aabayomi/huggingface/runs/1v4pv6el\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKZ2plXbih7P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7BOCoEliiA7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFa1hMoSiiGF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "676MIqIWiiMW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}